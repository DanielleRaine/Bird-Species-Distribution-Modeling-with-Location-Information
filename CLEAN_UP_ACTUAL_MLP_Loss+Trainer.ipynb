{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/DanielleRaine/Bird-Species-Distribution-Modeling-with-Location-Information/blob/main/CLEAN_UP_ACTUAL_MLP_Loss%2BTrainer.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sM-W2wT-SC-d",
        "outputId": "2b26f45e-543c-4676-9048-43ff115f8494"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting nbimporter\n",
            "  Downloading nbimporter-0.3.4-py3-none-any.whl.metadata (252 bytes)\n",
            "Downloading nbimporter-0.3.4-py3-none-any.whl (4.9 kB)\n",
            "Installing collected packages: nbimporter\n",
            "Successfully installed nbimporter-0.3.4\n",
            "Collecting import-ipynb\n",
            "  Downloading import_ipynb-0.2-py3-none-any.whl.metadata (2.3 kB)\n",
            "Requirement already satisfied: IPython in /usr/local/lib/python3.10/dist-packages (from import-ipynb) (7.34.0)\n",
            "Requirement already satisfied: nbformat in /usr/local/lib/python3.10/dist-packages (from import-ipynb) (5.10.4)\n",
            "Requirement already satisfied: setuptools>=18.5 in /usr/local/lib/python3.10/dist-packages (from IPython->import-ipynb) (75.1.0)\n",
            "Collecting jedi>=0.16 (from IPython->import-ipynb)\n",
            "  Downloading jedi-0.19.2-py2.py3-none-any.whl.metadata (22 kB)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.10/dist-packages (from IPython->import-ipynb) (4.4.2)\n",
            "Requirement already satisfied: pickleshare in /usr/local/lib/python3.10/dist-packages (from IPython->import-ipynb) (0.7.5)\n",
            "Requirement already satisfied: traitlets>=4.2 in /usr/local/lib/python3.10/dist-packages (from IPython->import-ipynb) (5.7.1)\n",
            "Requirement already satisfied: prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from IPython->import-ipynb) (3.0.48)\n",
            "Requirement already satisfied: pygments in /usr/local/lib/python3.10/dist-packages (from IPython->import-ipynb) (2.18.0)\n",
            "Requirement already satisfied: backcall in /usr/local/lib/python3.10/dist-packages (from IPython->import-ipynb) (0.2.0)\n",
            "Requirement already satisfied: matplotlib-inline in /usr/local/lib/python3.10/dist-packages (from IPython->import-ipynb) (0.1.7)\n",
            "Requirement already satisfied: pexpect>4.3 in /usr/local/lib/python3.10/dist-packages (from IPython->import-ipynb) (4.9.0)\n",
            "Requirement already satisfied: fastjsonschema>=2.15 in /usr/local/lib/python3.10/dist-packages (from nbformat->import-ipynb) (2.20.0)\n",
            "Requirement already satisfied: jsonschema>=2.6 in /usr/local/lib/python3.10/dist-packages (from nbformat->import-ipynb) (4.23.0)\n",
            "Requirement already satisfied: jupyter-core!=5.0.*,>=4.12 in /usr/local/lib/python3.10/dist-packages (from nbformat->import-ipynb) (5.7.2)\n",
            "Requirement already satisfied: parso<0.9.0,>=0.8.4 in /usr/local/lib/python3.10/dist-packages (from jedi>=0.16->IPython->import-ipynb) (0.8.4)\n",
            "Requirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=2.6->nbformat->import-ipynb) (24.2.0)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=2.6->nbformat->import-ipynb) (2024.10.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=2.6->nbformat->import-ipynb) (0.35.1)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=2.6->nbformat->import-ipynb) (0.21.0)\n",
            "Requirement already satisfied: platformdirs>=2.5 in /usr/local/lib/python3.10/dist-packages (from jupyter-core!=5.0.*,>=4.12->nbformat->import-ipynb) (4.3.6)\n",
            "Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.10/dist-packages (from pexpect>4.3->IPython->import-ipynb) (0.7.0)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.10/dist-packages (from prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0->IPython->import-ipynb) (0.2.13)\n",
            "Downloading import_ipynb-0.2-py3-none-any.whl (4.0 kB)\n",
            "Downloading jedi-0.19.2-py2.py3-none-any.whl (1.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m17.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: jedi, import-ipynb\n",
            "Successfully installed import-ipynb-0.2 jedi-0.19.2\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.5.1+cu121)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.16.1)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2024.10.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (3.0.2)\n",
            "Collecting torcheval\n",
            "  Downloading torcheval-0.0.7-py3-none-any.whl.metadata (8.6 kB)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torcheval) (4.12.2)\n",
            "Downloading torcheval-0.0.7-py3-none-any.whl (179 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m179.2/179.2 kB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: torcheval\n",
            "Successfully installed torcheval-0.0.7\n",
            "Collecting torchmetrics\n",
            "  Downloading torchmetrics-1.6.0-py3-none-any.whl.metadata (20 kB)\n",
            "Requirement already satisfied: numpy>1.20.0 in /usr/local/lib/python3.10/dist-packages (from torchmetrics) (1.26.4)\n",
            "Requirement already satisfied: packaging>17.1 in /usr/local/lib/python3.10/dist-packages (from torchmetrics) (24.2)\n",
            "Requirement already satisfied: torch>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from torchmetrics) (2.5.1+cu121)\n",
            "Collecting lightning-utilities>=0.8.0 (from torchmetrics)\n",
            "  Downloading lightning_utilities-0.11.9-py3-none-any.whl.metadata (5.2 kB)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from lightning-utilities>=0.8.0->torchmetrics) (75.1.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from lightning-utilities>=0.8.0->torchmetrics) (4.12.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=2.0.0->torchmetrics) (3.16.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=2.0.0->torchmetrics) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=2.0.0->torchmetrics) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=2.0.0->torchmetrics) (2024.10.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch>=2.0.0->torchmetrics) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch>=2.0.0->torchmetrics) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=2.0.0->torchmetrics) (3.0.2)\n",
            "Downloading torchmetrics-1.6.0-py3-none-any.whl (926 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m926.4/926.4 kB\u001b[0m \u001b[31m17.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading lightning_utilities-0.11.9-py3-none-any.whl (28 kB)\n",
            "Installing collected packages: lightning-utilities, torchmetrics\n",
            "Successfully installed lightning-utilities-0.11.9 torchmetrics-1.6.0\n",
            "Collecting comet-ml\n",
            "  Downloading comet_ml-3.47.3-py3-none-any.whl.metadata (3.9 kB)\n",
            "Collecting everett<3.2.0,>=1.0.1 (from everett[ini]<3.2.0,>=1.0.1->comet-ml)\n",
            "  Downloading everett-3.1.0-py2.py3-none-any.whl.metadata (17 kB)\n",
            "Requirement already satisfied: jsonschema!=3.1.0,>=2.6.0 in /usr/local/lib/python3.10/dist-packages (from comet-ml) (4.23.0)\n",
            "Requirement already satisfied: psutil>=5.6.3 in /usr/local/lib/python3.10/dist-packages (from comet-ml) (5.9.5)\n",
            "Collecting python-box<7.0.0 (from comet-ml)\n",
            "  Downloading python_box-6.1.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.8 kB)\n",
            "Requirement already satisfied: requests-toolbelt>=0.8.0 in /usr/local/lib/python3.10/dist-packages (from comet-ml) (1.0.0)\n",
            "Requirement already satisfied: requests>=2.18.4 in /usr/local/lib/python3.10/dist-packages (from comet-ml) (2.32.3)\n",
            "Collecting semantic-version>=2.8.0 (from comet-ml)\n",
            "  Downloading semantic_version-2.10.0-py2.py3-none-any.whl.metadata (9.7 kB)\n",
            "Requirement already satisfied: sentry-sdk>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from comet-ml) (2.18.0)\n",
            "Collecting simplejson (from comet-ml)\n",
            "  Downloading simplejson-3.19.3-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.2 kB)\n",
            "Requirement already satisfied: urllib3>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from comet-ml) (2.2.3)\n",
            "Requirement already satisfied: wrapt>=1.11.2 in /usr/local/lib/python3.10/dist-packages (from comet-ml) (1.16.0)\n",
            "Collecting wurlitzer>=1.0.2 (from comet-ml)\n",
            "  Downloading wurlitzer-3.1.1-py3-none-any.whl.metadata (2.5 kB)\n",
            "Collecting dulwich!=0.20.33,>=0.20.6 (from comet-ml)\n",
            "  Downloading dulwich-0.22.6-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.3 kB)\n",
            "Requirement already satisfied: rich>=13.3.2 in /usr/local/lib/python3.10/dist-packages (from comet-ml) (13.9.4)\n",
            "Collecting configobj (from everett[ini]<3.2.0,>=1.0.1->comet-ml)\n",
            "  Downloading configobj-5.0.9.tar.gz (101 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m101.5/101.5 kB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.10/dist-packages (from jsonschema!=3.1.0,>=2.6.0->comet-ml) (24.2.0)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.10/dist-packages (from jsonschema!=3.1.0,>=2.6.0->comet-ml) (2024.10.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.10/dist-packages (from jsonschema!=3.1.0,>=2.6.0->comet-ml) (0.35.1)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.10/dist-packages (from jsonschema!=3.1.0,>=2.6.0->comet-ml) (0.21.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.18.4->comet-ml) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.18.4->comet-ml) (3.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.18.4->comet-ml) (2024.8.30)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich>=13.3.2->comet-ml) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich>=13.3.2->comet-ml) (2.18.0)\n",
            "Requirement already satisfied: typing-extensions<5.0,>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from rich>=13.3.2->comet-ml) (4.12.2)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich>=13.3.2->comet-ml) (0.1.2)\n",
            "Downloading comet_ml-3.47.3-py3-none-any.whl (709 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m709.7/709.7 kB\u001b[0m \u001b[31m18.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading dulwich-0.22.6-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (981 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m981.8/981.8 kB\u001b[0m \u001b[31m38.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading everett-3.1.0-py2.py3-none-any.whl (35 kB)\n",
            "Downloading python_box-6.1.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.3/3.3 MB\u001b[0m \u001b[31m33.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading semantic_version-2.10.0-py2.py3-none-any.whl (15 kB)\n",
            "Downloading wurlitzer-3.1.1-py3-none-any.whl (8.6 kB)\n",
            "Downloading simplejson-3.19.3-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (137 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m137.9/137.9 kB\u001b[0m \u001b[31m10.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: configobj\n",
            "  Building wheel for configobj (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for configobj: filename=configobj-5.0.9-py2.py3-none-any.whl size=35615 sha256=636535b4b2632a9004b2effdad0599505488d97d81e9d9958488e190be079a32\n",
            "  Stored in directory: /root/.cache/pip/wheels/a1/6c/03/6c5e3cf1a6e4b9e2fc5c4409be4abc5a8268bd9c878739cb32\n",
            "Successfully built configobj\n",
            "Installing collected packages: everett, wurlitzer, simplejson, semantic-version, python-box, dulwich, configobj, comet-ml\n",
            "  Attempting uninstall: python-box\n",
            "    Found existing installation: python-box 7.2.0\n",
            "    Uninstalling python-box-7.2.0:\n",
            "      Successfully uninstalled python-box-7.2.0\n",
            "Successfully installed comet-ml-3.47.3 configobj-5.0.9 dulwich-0.22.6 everett-3.1.0 python-box-6.1.0 semantic-version-2.10.0 simplejson-3.19.3 wurlitzer-3.1.1\n"
          ]
        }
      ],
      "source": [
        "!pip install nbimporter\n",
        "!pip install import-ipynb\n",
        "!pip install torch\n",
        "!pip install torcheval\n",
        "!pip install torchmetrics\n",
        "!pip install comet-ml"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uuGHkGv5BDHt",
        "outputId": "d45f3f8d-591f-48de-8209-c1ca4a150af1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "import sys\n",
        "import os\n",
        "import json\n",
        "import pickle\n",
        "from typing import Any\n",
        "from ctypes import sizeof\n",
        "\n",
        "from comet_ml import Experiment\n",
        "from comet_ml.integration.pytorch import log_model\n",
        "\n",
        "import torch\n",
        "import torch.utils.data\n",
        "from torch.utils.data import Dataset\n",
        "from torch.utils.data import DataLoader\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "from torcheval.metrics import MeanSquaredError\n",
        "from torchmetrics.regression import MeanAbsoluteError\n",
        "from torchmetrics.classification import Accuracy\n",
        "\n",
        "from google.colab import userdata\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2awrCJN95kt5"
      },
      "outputs": [],
      "source": [
        "class BirdHotspotsDataset(Dataset):\n",
        "  \"\"\"\n",
        "  Dataset for Bird Species Distribution Modeling with Location Information (Bird Hotspots).\n",
        "  \"\"\"\n",
        "\n",
        "  def __init__(self, features_df, targets_df):\n",
        "    \"\"\"\n",
        "    Args:\n",
        "      features_df (pd.DataFrame): DataFrame containing the features.\n",
        "      targets_df (pd.DataFrame): DataFrame containing the targets.\n",
        "    \"\"\"\n",
        "\n",
        "    self.data = [\n",
        "        [torch.tensor(pd.to_numeric(row.drop(labels = [\"hotspot_id\"]).values, errors = \"coerce\")).squeeze(),\n",
        "         torch.tensor(targets_df.loc[targets_df[\"hotspot_id\"] == row[\"hotspot_id\"]]\n",
        "                                    .drop(columns = [\"hotspot_id\", \"num_complete_checklists\"]).values).squeeze()]\n",
        "                 for i, row in features_df.iterrows()]\n",
        "\n",
        "  def __len__(self):\n",
        "    \"\"\"\n",
        "    Returns the length of the dataset.\n",
        "    Returns:\n",
        "      length (int): Length of the dataset.\n",
        "    \"\"\"\n",
        "\n",
        "    return len(self.data)\n",
        "\n",
        "  def __getitem__(self, idx):\n",
        "    \"\"\"\n",
        "    Returns the record and its target.\n",
        "    Args:\n",
        "      idx (int): Index of the record.\n",
        "    Returns:\n",
        "      record (torch.Tensor): Record.\n",
        "      target (torch.Tensor): Target.\n",
        "    \"\"\"\n",
        "\n",
        "    return self.data[idx]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ua2PbJ0vEros"
      },
      "outputs": [],
      "source": [
        "# The next two cells are for turning the data into datasets that the model can use.\n",
        "# The third cell is for pickling the dataset for future use, as preprocessing can take a while.\n",
        "# The the fourth is for loading the pickled data sets. Skip to the fourth if already done so."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E4JRi6FWGB55"
      },
      "outputs": [],
      "source": [
        "# The data used for the model using un-augmented data\n",
        "# training_df = pd.read_csv(\"/content/drive/MyDrive/TeamMila/ProjectDataset/train_split.csv\")\n",
        "# evaluation_df = pd.read_csv(\"/content/drive/MyDrive/TeamMila/ProjectDataset/valid_split.csv\")\n",
        "# testing_df = pd.read_csv(\"/content/drive/MyDrive/TeamMila/ProjectDataset/test_split.csv\")\n",
        "# targets_df = pd.read_csv(\"/content/drive/MyDrive/TeamMila/ProjectDataset/targets.csv\")\n",
        "\n",
        "# The data used for the model using augmented data\n",
        "# training_df = pd.read_csv(\"/content/drive/MyDrive/TeamMila/ProjectDataset/normalized_train.csv\")\n",
        "# evaluation_df = pd.read_csv(\"/content/drive/MyDrive/TeamMila/ProjectDataset/normalized_val.csv\")\n",
        "# testing_df = pd.read_csv(\"/content/drive/MyDrive/TeamMila/ProjectDataset/normalized_test.csv\")\n",
        "# targets_df = pd.read_csv(\"/content/drive/MyDrive/TeamMila/ProjectDataset/targets.csv\")\n",
        "\n",
        "# Columns that will be used for the data\n",
        "# df_columns = [f\"bio_{i}\" for i in range(1, 20)] + [\"hotspot_id\", \"orcdrc\", \"phihox\", \"cecsol\", \"bdticm\", \"clyppt\", \"sltppt\", \"sndppt\", \"bldfie\"]\n",
        "\n",
        "# training_df = training_df[df_columns]\n",
        "# evaluation_df = evaluation_df[df_columns]\n",
        "# testing_df = testing_df[df_columns]"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Turn the data into pytorch datasets\n",
        "# training_set = BirdHotspotsDataset(training_df, targets_df)\n",
        "# evaluation_set = BirdHotspotsDataset(evaluation_df, targets_df)\n",
        "# testing_set = BirdHotspotsDataset(testing_df, targets_df)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oidxqsh73mFG",
        "outputId": "ff4fbbc2-a513-44d6-81ef-114fef6179d1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Index(['hotspot_id', 'lon', 'lat', 'county', 'county_code', 'state',\n",
              "       'state_code', 'num_complete_checklists', 'num_different_species',\n",
              "       'bio_1', 'bio_2', 'bio_3', 'bio_4', 'bio_5', 'bio_6', 'bio_7', 'bio_8',\n",
              "       'bio_9', 'bio_10', 'bio_11', 'bio_12', 'bio_13', 'bio_14', 'bio_15',\n",
              "       'bio_16', 'bio_17', 'bio_18', 'bio_19', 'bdticm', 'bldfie', 'cecsol',\n",
              "       'clyppt', 'orcdrc', 'phihox', 'sltppt', 'sndppt', 'split'],\n",
              "      dtype='object')"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-bzL2JHAVLHn"
      },
      "outputs": [],
      "source": [
        "# with open(\"/content/drive/MyDrive/TeamMila/ProjectDataset/normalized_train.p\", \"wb\") as f:\n",
        "#     pickle.dump(training_set, f)\n",
        "\n",
        "# with open(\"/content/drive/MyDrive/TeamMila/ProjectDataset/normalized_val.p\", \"wb\") as f:\n",
        "#     pickle.dump(evaluation_set, f)\n",
        "\n",
        "# with open(\"/content/drive/MyDrive/TeamMila/ProjectDataset/normalized_test.p\", \"wb\") as f:\n",
        "#     pickle.dump(testing_set, f)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Unpickling the augmented datasets.\n",
        "with open(\"/content/drive/MyDrive/TeamMila/ProjectDataset/normalized_train.p\", \"rb\") as f:\n",
        "  training_set = pickle.load(f)\n",
        "\n",
        "with open(\"/content/drive/MyDrive/TeamMila/ProjectDataset/normalized_val.p\", \"rb\") as f:\n",
        "  evaluation_set = pickle.load(f)\n",
        "\n",
        "with open(\"/content/drive/MyDrive/TeamMila/ProjectDataset/normalized_test.p\", \"rb\") as f:\n",
        "  testing_set = pickle.load(f)\n",
        "\n",
        "# Unpickling the un-augmented datasets.\n",
        "# with open(\"/content/drive/MyDrive/TeamMila/ProjectDataset/train_split.p\", \"rb\") as f:\n",
        "#     training_set = pickle.load(f)\n",
        "\n",
        "# with open(\"/content/drive/MyDrive/TeamMila/ProjectDataset/valid_split.p\", \"rb\") as f:\n",
        "#   evaluation_set = pickle.load(f)\n",
        "\n",
        "# with open(\"/content/drive/MyDrive/TeamMila/ProjectDataset/test_split.p\", \"rb\") as f:\n",
        "#   testing_set = pickle.load(f)"
      ],
      "metadata": {
        "id": "snRT2RebKgXl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kx0UBoRmCmSl"
      },
      "outputs": [],
      "source": [
        "# different datloaders for the splits of the data\n",
        "train_dataloader = DataLoader(training_set, batch_size=64, shuffle=True)\n",
        "valid_dataloader = DataLoader(evaluation_set, batch_size=64, shuffle=True)\n",
        "test_dataloader = DataLoader(testing_set, batch_size=64, shuffle=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_HUVYTBPIyAu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1bbee650-8ddc-401f-8341-447fd928dc3a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([64, 27]) torch.Size([64, 671]) 27 671\n"
          ]
        }
      ],
      "source": [
        "X_train, y_train = next(iter(train_dataloader))\n",
        "print(X_train.shape, y_train.shape, len(X_train[0]), len(y_train[0]))\n",
        "# x, y = next(iter(valid_dataloader))\n",
        "# print(x.shape, y.shape, len(x[0]), len(y[0]))\n",
        "# x, y = next(iter(test_dataloader))\n",
        "# print(x.shape, y.shape, len(x[0]), len(y[0]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XK1FOfwCE1hm"
      },
      "outputs": [],
      "source": [
        "class EncounterRateMLP(torch.nn.Module):\n",
        "  def __init__(self,num_inputs, num_classes, hidden_dimensions=128,ebd=False):\n",
        "\n",
        "    #num\n",
        "      super(EncounterRateMLP, self).__init__()\n",
        "      self.inc_bias = False\n",
        "      self.feats = nn.Sequential(\n",
        "          nn.Linear(num_inputs, hidden_dimensions),\n",
        "          nn.LeakyReLU(inplace=True),\n",
        "          nn.Linear(hidden_dimensions, num_classes),\n",
        "          nn.LeakyReLU(inplace=True)\n",
        "      )\n",
        "  def forward(self, x, class_of_interest=None, return_feats=False):\n",
        "      return torch.sigmoid(self.feats(x))\n",
        "\n",
        "# evaluates a single class\n",
        "  def eval_single_class(self, x, class_of_interest):\n",
        "      if self.inc_bias:\n",
        "        #dot product\n",
        "          return torch.matmul(x, self.class_emb.weight[class_of_interest, :]) + self.class_emb.bias[class_of_interest]\n",
        "      else:\n",
        "          return torch.matmul(x, self.class_emb.weight[class_of_interest, :])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y4HQUbuJcA-3"
      },
      "outputs": [],
      "source": [
        "def TopKAccuracy(outputs, targets, k = None):\n",
        "    if k is None:\n",
        "        # Let K be the number of nonzero values for a set of predictions\n",
        "        sum_correct = 0\n",
        "        batch_size = outputs.shape[0]\n",
        "        for output, target in zip(outputs, targets):\n",
        "            k = torch.count_nonzero(target).item()\n",
        "            top_k_preds = torch.topk(output, k, dim=0).indices\n",
        "            true_labels = torch.topk(target, k, dim=0).indices\n",
        "            sum_correct += torch.any(top_k_preds == true_labels)\n",
        "        correct = sum_correct / batch_size\n",
        "        return correct\n",
        "\n",
        "    # Get the top K predictions' indices\n",
        "    top_k_preds = torch.topk(outputs, k, dim=1).indices\n",
        "    true_labels = torch.topk(targets, k, dim=1).indices\n",
        "\n",
        "    # Check if the true label is in the top K predictions\n",
        "    correct = torch.any(top_k_preds == true_labels, dim=1)\n",
        "\n",
        "    # Return the average success of the batch of predictions\n",
        "    return correct.float().mean().item()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LU2W-ChlR8or"
      },
      "outputs": [],
      "source": [
        "def training_step(model, dataloader, eval_dataloader, criterion, optimizer, device, checkpoint_dir, num_epochs, experiment = None):\n",
        "    model.to(device)\n",
        "    best_loss = float('inf')\n",
        "    last_checkpoint_path = os.path.join(checkpoint_dir, 'last_checkpoint.pth')\n",
        "    best_checkpoint_path = os.path.join(checkpoint_dir, 'best_checkpoint.pth')\n",
        "    mse_metric = MeanSquaredError().to(device)\n",
        "    mae_metric = MeanAbsoluteError().to(device)\n",
        "    mse_metric_eval = MeanSquaredError().to(device)\n",
        "    mae_metric_eval = MeanAbsoluteError().to(device)\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        model.train()\n",
        "        mse_metric.reset()\n",
        "        mae_metric.reset()\n",
        "        mse_metric_eval.reset()\n",
        "        mae_metric_eval.reset()\n",
        "\n",
        "        print(\"A NEW EPOCH HAS STARTED\")\n",
        "\n",
        "        running_loss = 0.0\n",
        "\n",
        "        top_10_correct = 0\n",
        "        top_30_correct = 0\n",
        "        top_k_correct = 0\n",
        "        num_batches = 0\n",
        "\n",
        "        top_10_correct_eval = 0\n",
        "        top_30_correct_eval = 0\n",
        "        top_k_correct_eval = 0\n",
        "        num_batches_eval = 0\n",
        "\n",
        "        for inputs, targets in dataloader:\n",
        "          ### runtime error of having inputs and targets as non-floats\n",
        "          inputs, targets = inputs.float().to(device), targets.float().to(device)\n",
        "\n",
        "          optimizer.zero_grad()\n",
        "\n",
        "          outputs = model(inputs)\n",
        "\n",
        "          #runtime error address: targets not in between 0 to 1\n",
        "          targets = torch.clamp(targets,0,1)\n",
        "          loss = criterion(outputs, targets)\n",
        "\n",
        "          loss.backward()\n",
        "          optimizer.step()\n",
        "\n",
        "          running_loss += loss.item()\n",
        "\n",
        "          mse_metric.update(outputs, targets)\n",
        "          mae_metric.update(outputs, targets)\n",
        "\n",
        "          top_10_correct += TopKAccuracy(outputs, targets, k = 10)\n",
        "          top_30_correct += TopKAccuracy(outputs, targets, k = 30)\n",
        "          top_k_correct += TopKAccuracy(outputs, targets)\n",
        "          num_batches += 1\n",
        "\n",
        "        model.eval()\n",
        "\n",
        "        for inputs, targets in eval_dataloader:\n",
        "          with torch.no_grad():\n",
        "            inputs, targets = inputs.float().to(device), targets.float().to(device)\n",
        "            outputs = model(inputs)\n",
        "            targets = torch.clamp(targets,0,1)\n",
        "\n",
        "            mse_metric_eval.update(outputs, targets)\n",
        "            mae_metric_eval.update(outputs, targets)\n",
        "\n",
        "            top_10_correct_eval += TopKAccuracy(outputs, targets, k = 10)\n",
        "            top_30_correct_eval += TopKAccuracy(outputs, targets, k = 30)\n",
        "            top_k_correct_eval += TopKAccuracy(outputs, targets)\n",
        "            num_batches_eval += 1\n",
        "\n",
        "        mse = mse_metric.compute()\n",
        "        mae = mae_metric.compute()\n",
        "\n",
        "        top_10 = top_10_correct / num_batches\n",
        "        top_30 = top_30_correct / num_batches\n",
        "        top_k = top_k_correct / num_batches\n",
        "\n",
        "        mse_eval = mse_metric_eval.compute()\n",
        "        mae_eval = mae_metric_eval.compute()\n",
        "\n",
        "        top_10_correct_eval = top_10_correct_eval / num_batches_eval\n",
        "        top_30_correct_eval = top_30_correct_eval / num_batches_eval\n",
        "        top_k_correct_eval = top_k_correct_eval / num_batches_eval\n",
        "\n",
        "        if experiment is not None:\n",
        "          experiment.log_metrics({\n",
        "              \"mse\": mse,\n",
        "              \"mae\": mae,\n",
        "              \"top_10_accuracy\": top_10,\n",
        "              \"top_30_accuracy\": top_30,\n",
        "              \"top_k_accuracy\": top_k,\n",
        "              \"mse_eval\": mse_eval,\n",
        "              \"mae_eval\": mae_eval,\n",
        "              \"top_10_accuracy_eval\": top_10_correct_eval,\n",
        "              \"top_30_accuracy_eval\": top_30_correct_eval,\n",
        "              \"top_k_accuracy_eval\": top_k_correct_eval\n",
        "              }, step=epoch\n",
        "          )\n",
        "\n",
        "        print(f\"Epoch [{epoch + 1}/{num_epochs}], \" f\"Mean Squared Error: {mse.item():.5f}, \" f\"Mean Absolute Error: {mae.item():.5f}, \" f\"Top 10 Accuracy: {top_10:.5f}, \" f\"Top 30 Accuracy: {top_30:.5f},\" f\" Top K Accuracy: {top_k:.5f}\")\n",
        "\n",
        "        #-----\n",
        "        #below statement has been tested and does work\n",
        "        # torch.save(model.state_dict(), last_checkpoint_path)\n",
        "        #-----\n",
        "\n",
        "        checkpoint = {\n",
        "        'epoch': epoch + 1,  # Save the epoch number (1-based index)\n",
        "        'model_state_dict': model.state_dict(),\n",
        "        'optimizer_state_dict': optimizer.state_dict(),\n",
        "        'loss': loss.item()\n",
        "        }\n",
        "\n",
        "        torch.save(checkpoint, f\"checkpoints_epoch_{epoch+1}.pth\")\n",
        "\n",
        "    epoch_loss = running_loss / len(dataloader)\n",
        "    if epoch_loss < best_loss:\n",
        "        best_loss = epoch_loss\n",
        "        torch.save(model.state_dict(), best_checkpoint_path)\n",
        "        print(f'CURRENT BEST MODEL: {epoch + 1} LOSS: {best_loss:.5f}')\n",
        "\n",
        "\n",
        "    print(f'CURRENT EPOCH: [{epoch + 1}/{num_epochs}], LOSS: {epoch_loss:.5f}')\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def testing_step(model, dataloader, device, experiment = None):\n",
        "    model.to(device)\n",
        "\n",
        "    mse_metric = MeanSquaredError().to(device)\n",
        "    mae_metric = MeanAbsoluteError().to(device)\n",
        "\n",
        "    mse_metric.reset()\n",
        "    mae_metric.reset()\n",
        "\n",
        "    top_10_correct = 0\n",
        "    top_30_correct = 0\n",
        "    top_k_correct = 0\n",
        "    num_batches = 0\n",
        "\n",
        "    model.eval()\n",
        "\n",
        "    for inputs, targets in dataloader:\n",
        "      inputs, targets = inputs.float().to(device), targets.float().to(device)\n",
        "      outputs = model(inputs)\n",
        "      targets = torch.clamp(targets,0,1)\n",
        "\n",
        "      mse_metric.update(outputs, targets)\n",
        "      mae_metric.update(outputs, targets)\n",
        "\n",
        "      top_10_correct += TopKAccuracy(outputs, targets, k = 10)\n",
        "      top_30_correct += TopKAccuracy(outputs, targets, k = 30)\n",
        "      top_k_correct += TopKAccuracy(outputs, targets)\n",
        "      num_batches += 1\n",
        "\n",
        "    mse = mse_metric.compute()\n",
        "    mae = mae_metric.compute()\n",
        "\n",
        "    top_10 = top_10_correct / num_batches\n",
        "    top_30 = top_30_correct / num_batches\n",
        "    top_k = top_k_correct / num_batches\n",
        "\n",
        "    print(f\"Mean Squared Error: {mse.item():.5f}, Mean Absolute Error: {mae.item():.5f}\")\n",
        "\n",
        "    print(f\"Top 10 Accuracy: {top_10:.5f}, Top 30 Accuracy: {top_30:.5f}, Top K Accuracy: {top_k:.5f}\")\n",
        "\n",
        "    if experiment is not None:\n",
        "      experiment.log_metrics({\n",
        "          \"mse_test\": mse,\n",
        "          \"mae_test\": mae,\n",
        "          \"top_10_accuracy_test\": top_10,\n",
        "          \"top_30_accuracy_test\": top_30,\n",
        "          \"top_k_accuracy_test\": top_k\n",
        "      })"
      ],
      "metadata": {
        "id": "pqKlUPz2n1Jb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SDo724uAAPG0"
      },
      "outputs": [],
      "source": [
        "def CrossEntropyLoss(model,predictions,targets):\n",
        "  criterion = nn.BCELoss()\n",
        "  loss = criterion(predictions, targets)\n",
        "  return loss\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U35U9s2RcVN9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "34b43348-c3d4-47a2-8078-03bb552227e1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[1;38;5;214mCOMET WARNING:\u001b[0m As you are running in a Jupyter environment, you will need to call `experiment.end()` when finished to ensure all metrics and code are logged before exiting.\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m Couldn't find a Git repository in '/content' nor in any parent directory. Set `COMET_GIT_DIRECTORY` if your Git Repository is elsewhere.\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m Experiment is live on comet.com https://www.comet.com/danielleraine/bird-species-distribution-modeling-with-location-information/6f9305b33a3a42af825afcb36ac599b1\n",
            "\n"
          ]
        }
      ],
      "source": [
        "experiment = Experiment(\n",
        "  api_key=userdata.get('comet_api_key'),\n",
        "  project_name=\"bird-species-distribution-modeling-with-location-information\",\n",
        "  workspace=\"danielleraine\"\n",
        ")\n",
        "\n",
        "experiment.set_name(\"baseline-augnorm\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R2BgP1xUdlnw"
      },
      "outputs": [],
      "source": [
        "hyper_params = {\n",
        "    'batch_size': 64,\n",
        "    'learning_rate': 0.001,\n",
        "    'num_epochs': 10,\n",
        "}\n",
        "\n",
        "experiment.log_parameters(hyper_params)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = EncounterRateMLP(27, 671, hidden_dimensions=128,ebd=False).float()\n",
        "criterion = nn.BCELoss()"
      ],
      "metadata": {
        "id": "ieKnfFvWMfBJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aqygLQFrKU0l",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "40aaba0a-32b9-4d26-88fa-28645bd2643b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "A NEW EPOCH HAS STARTED\n",
            "Epoch [1/10], Mean Squared Error: 0.04850, Mean Absolute Error: 0.13728, Top 10 Accuracy: 0.72028, Top 30 Accuracy: 0.77307, Top K Accuracy: 0.77812\n",
            "A NEW EPOCH HAS STARTED\n",
            "Epoch [2/10], Mean Squared Error: 0.00935, Mean Absolute Error: 0.03181, Top 10 Accuracy: 0.74429, Top 30 Accuracy: 0.80304, Top K Accuracy: 0.81358\n",
            "A NEW EPOCH HAS STARTED\n",
            "Epoch [3/10], Mean Squared Error: 0.00934, Mean Absolute Error: 0.03085, Top 10 Accuracy: 0.74298, Top 30 Accuracy: 0.80334, Top K Accuracy: 0.81269\n",
            "A NEW EPOCH HAS STARTED\n",
            "Epoch [4/10], Mean Squared Error: 0.00934, Mean Absolute Error: 0.03070, Top 10 Accuracy: 0.74314, Top 30 Accuracy: 0.80199, Top K Accuracy: 0.81158\n",
            "A NEW EPOCH HAS STARTED\n",
            "Epoch [5/10], Mean Squared Error: 0.00931, Mean Absolute Error: 0.03061, Top 10 Accuracy: 0.74419, Top 30 Accuracy: 0.80390, Top K Accuracy: 0.81163\n",
            "A NEW EPOCH HAS STARTED\n",
            "Epoch [6/10], Mean Squared Error: 0.00914, Mean Absolute Error: 0.03022, Top 10 Accuracy: 0.73985, Top 30 Accuracy: 0.80521, Top K Accuracy: 0.81447\n",
            "A NEW EPOCH HAS STARTED\n",
            "Epoch [7/10], Mean Squared Error: 0.00878, Mean Absolute Error: 0.02913, Top 10 Accuracy: 0.73452, Top 30 Accuracy: 0.81373, Top K Accuracy: 0.82213\n",
            "A NEW EPOCH HAS STARTED\n",
            "Epoch [8/10], Mean Squared Error: 0.00859, Mean Absolute Error: 0.02837, Top 10 Accuracy: 0.73744, Top 30 Accuracy: 0.81509, Top K Accuracy: 0.82337\n",
            "A NEW EPOCH HAS STARTED\n",
            "Epoch [9/10], Mean Squared Error: 0.00846, Mean Absolute Error: 0.02793, Top 10 Accuracy: 0.73818, Top 30 Accuracy: 0.81736, Top K Accuracy: 0.82567\n",
            "A NEW EPOCH HAS STARTED\n",
            "Epoch [10/10], Mean Squared Error: 0.00833, Mean Absolute Error: 0.02750, Top 10 Accuracy: 0.73903, Top 30 Accuracy: 0.82002, Top K Accuracy: 0.82897\n",
            "CURRENT BEST MODEL: 10 LOSS: 0.06652\n",
            "CURRENT EPOCH: [10/10], LOSS: 0.06652\n"
          ]
        }
      ],
      "source": [
        "if torch.cuda.is_available():\n",
        "    device = 'cuda'\n",
        "else:\n",
        "    device = 'cpu'\n",
        "# optimizer = torch.optim.SGD(model.parameters(), lr=0.01, momentum=0.9)\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
        "checkpoint_dir = './checkpoints'\n",
        "os.makedirs(checkpoint_dir, exist_ok=True)\n",
        "\n",
        "last_checkpoint_path = os.path.join(checkpoint_dir, 'last_checkpoint.pth')\n",
        "best_checkpoint_path = os.path.join(checkpoint_dir, 'best_checkpoint.pth')\n",
        "\n",
        "# Save model state after each epoch\n",
        "# torch.save(model.state_dict(),\n",
        "#            last_checkpoint_path)\n",
        "loss = 0.0\n",
        "epoch = 0\n",
        "torch.save({\n",
        "    'epoch': epoch,\n",
        "    'model_state_dict': model.state_dict(),\n",
        "    'optimizer_state_dict': optimizer.state_dict(),\n",
        "    'loss': loss,\n",
        "}, \"best_checkpoint.pth\")\n",
        "\n",
        "training_step(\n",
        "    model=model,\n",
        "    dataloader=train_dataloader,\n",
        "    eval_dataloader=valid_dataloader,\n",
        "    criterion=criterion,\n",
        "    optimizer=optimizer,\n",
        "    device=device,\n",
        "    checkpoint_dir=checkpoint_dir,\n",
        "    num_epochs=10,  # Number of epochs for training\n",
        "    experiment=experiment\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CFVCwR62TdAq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e141da1a-d3d0-4f2a-ac28-5b09f8db9880"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "odict_keys(['feats.0.weight', 'feats.0.bias', 'feats.2.weight', 'feats.2.bias'])\n",
            "tensor([[-0.6991, -0.1071,  0.4424,  ...,  2.3730,  2.6345,  0.3510],\n",
            "        [-0.4678, -0.1232, -0.8607,  ...,  2.9040,  1.8779,  0.9207],\n",
            "        [ 1.3438, -0.9566, -0.9068,  ...,  1.3304,  0.7855,  1.0264],\n",
            "        ...,\n",
            "        [ 0.0111, -1.1264,  0.4330,  ...,  1.7392,  2.5955,  0.2371],\n",
            "        [ 0.4334, -0.4779,  0.0314,  ..., -1.8369, -1.5135, -0.5403],\n",
            "        [ 0.3201,  0.4476,  0.8273,  ...,  1.0665,  1.9521,  1.0294]])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-57-121b72e06327>:1: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  checkpoint = torch.load(\"checkpoints/best_checkpoint.pth\")\n"
          ]
        }
      ],
      "source": [
        "checkpoint = torch.load(\"checkpoints/best_checkpoint.pth\")\n",
        "print(checkpoint.keys())\n",
        "print(checkpoint['feats.0.weight'])"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# torch.save(model, \"/content/drive/MyDrive/TeamMila/Models/baseline-augnorm.pth\")"
      ],
      "metadata": {
        "id": "G3aQAJDCm5Je"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# model = torch.load(\"/content/drive/MyDrive/TeamMila/Models/baseline_model.pth\", weights_only=False)"
      ],
      "metadata": {
        "id": "sePuiK61nkUW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "testing_step(model, test_dataloader, device, experiment)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IluxScDysxMr",
        "outputId": "910d0349-0a72-4d4c-a35f-d32537459d10"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mean Squared Error: 0.00863, Mean Absolute Error: 0.02875\n",
            "Top 10 Accuracy: 0.71888, Top 30 Accuracy: 0.80734, Top K Accuracy: 0.81478\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bE96BbvUGUd3"
      },
      "outputs": [],
      "source": [
        "log_model(experiment, model, \"baseline-augnorm\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hima5cY9g8u_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "38f05020-e7d9-4ffd-ca44-bc702d882727"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m ---------------------------------------------------------------------------------------\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m Comet.ml Experiment Summary\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m ---------------------------------------------------------------------------------------\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m   Data:\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     display_summary_level : 1\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     name                  : baseline-augnorm\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     url                   : https://www.comet.com/danielleraine/bird-species-distribution-modeling-with-location-information/6f9305b33a3a42af825afcb36ac599b1\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m   Metrics [count] (min, max):\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     loss [1337]               : (0.05714833736419678, 0.7597743272781372)\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     mae [10]                  : (0.02687237039208412, 0.1361050009727478)\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     mae_eval [10]             : (0.028570998460054398, 0.035003166645765305)\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     mae_test                  : 0.028650352731347084\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     mse [10]                  : (0.00815696083009243, 0.048553768545389175)\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     mse_eval [10]             : (0.008615105412900448, 0.009654331021010876)\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     mse_test                  : 0.008536034263670444\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     top_10_accuracy [10]      : (0.714594336258625, 0.742898356049651)\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     top_10_accuracy_eval [10] : (0.7073239943076824, 0.7334446839217482)\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     top_10_accuracy_test      : 0.7180854885742582\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     top_30_accuracy [10]      : (0.7714711679570605, 0.8225552562287073)\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     top_30_accuracy_eval [10] : (0.7921875, 0.8070545977559583)\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     top_30_accuracy_test      : 0.8035854232722315\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     top_k_accuracy [10]       : (0.7766636610031128, 0.8330453634262085)\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     top_k_accuracy_eval [10]  : (0.795714795589447, 0.8124030232429504)\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     top_k_accuracy_test       : 0.8126910328865051\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m   Others:\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     Name         : baseline-augnorm\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     notebook_url : https://colab.research.google.com/notebook#fileId=1jDRW-NBcoB_Aicr79IgjK9MSikuhvFHP\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m   Parameters:\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     batch_size    : 64\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     learning_rate : 0.001\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     num_epochs    : 10\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m   Uploads:\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     environment details : 1\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     filename            : 1\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     installed packages  : 1\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     model graph         : 1\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     model-element       : 2 (354.14 KB)\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     notebook            : 2\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     os packages         : 1\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     source_code         : 1\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m \n"
          ]
        }
      ],
      "source": [
        "experiment.end()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}